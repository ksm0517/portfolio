안녕하십니까 컴퓨터구조를 이해하고, 컴퓨터를 통해 사람들을 어려움으로 부터 구조하고 싶은
컴퓨터 구조의 오이프로젝트 OCR in editor의 발표를 맡게된 김성민입니다. (넘겨)
발표는 팀소개 최종 프로젝트로 만들어진 프로덕트를
필요와 이룬 것으로 나누어 소개하겠습니다. (넘겨)

팀구성원은 다음과 같습니다.(넘겨)
각자의 역할은
크게 PM, 서버, 모델링, front end 4개의 파트로 나누어 작업을 배분하고,
맡은 파트가 아니여도 서로 도와가며 프로젝트를 진행 하였습니다.(넘겨)

먼저 니즈에 대한 소개입니다.
일본영화를 보던 중 주인공이 아파서 약을 먹는데 약병의 글씨가 일본어로 되어 있었습니다.
그 때 저 약병은 뭐지? 라는 생각이 들었습니다.
하지만 일본어를 읽을 수 없어 아쉽다는 생각을 하였습니다.(넘겨)

여기보이는 이미지는 일본 드라마 고독한 미식가 중 한 장면인데요
원래는 일본어로 되어 있던 메뉴판을 한글화 시켜준 내용입니다.
이처럼 자막을 해주면 좋은데 모든 드라마 영화에서
저런 부분을 적용해주지 못하는 점이 의문이었습니다.
(넘겨)

기술 조사를 해본 결과 이미 파파고에서 
일상에서 볼 수 있는 외국어 메뉴판 등을
사진으로 찍으면 번역된 글씨를 반환해주는 툴이 있음을
확인했습니다.(넘겨)

하지만 여전히 영상 편집 툴에선 이를 해주는 기능이 없음을 확인했습니다.
즉, 고독한 미식가의 편집자 분들은 영상에 일일히 자막을 달고 계신 것이었습니다.
고생하시는 편집자분들의 퇴근시간을 구조해드리고 싶었습니다.(넘겨)

초기 설정한 목표입니다.
기능구현 UI측면으로는 편집 중 버튼을 눌렀을 때 서버로 이미지가 가고 이미지를 번역한 결과값, 및 bbox를 받아와서 시각화 하는 것
정확도로 Fscore 0.6이상
총 기능 모두가 수행 되는 데에 10초 이내의 목표를 설정했고
개발 중 시간이 남는 다면 다음과 같은 develop방향을 설정하였습니다.(넘겨)

먼저 저희가 만든 기능의 데모 영상 입니다.

~~~~(2분 정도)

기능의 작동 방식을 소개해드리겠습니다.
사용 버튼을 눌렀을 때,
서버로 프레임이 전송됩니다.
모델서버에서 글자 영역 검출 및 글자 추출을 수행합니다.
그리고 추출된 글자를 이용해 papagoAPI를 통해 번역합니다.
이렇게 나온 글자 영역 및 번역된 자막을 다시 front로 전송해주면 front는 이를 받아 보여주는 방식입니다.(넘겨)

조금 더 구체적으로 말씀을 드리 겠습니다.

영상 편집 툴은
코드가 pyqt5로 작성되어 있는 openshot을 사용했습니다.
비록 기업에서 사용할 때 pyqt는 유료이지만 접근이 쉬워 front를 이해하는 것에 좋으며,
opensource program을 변형해보는 경험을 쌓고자 선정하였습니다.
저희가 구현한 기능은 vidcap이라는 dock UI를 생성하고,
번역된 글들을 list로 보여주는 tab을 통해 listview를 구현하였습니다.
listview에 있는 checkbox를 켜고 끔을 통해 편집자분께서 사용할 자막을 고를 수 있게 하였습니다.(넘겨)

다음은 서버관련 내용입니다.
Model Serving은 FastAPI를 사용하여 Online Serving의 형태로 진행하였습니다.
서버의 흐름은 다음과 같습니다.
Openshot에서 Image를 request하면 총 세 단계를 걸쳐 Response하게 됩니다.
첫 번째, Image를 OCR Model이 Inference하여 Text와 Bounding Box를 얻습니다.
두 번째, 얻은 Text를 번역하는데 여기선 앞서 말씀드린데로 Papago API를 사용하였고 
향후 자체적인 모델로 변경 가능 할것으로 예상됩니다.
세 번째, Bounding Box 내의 Text색상과 Background 색상을 계산합니다.
그렇게 Bounding Box와 번역된 문장 그리고 색 총 3개의 정보를 Response합니다.
  
그리고 저희는 효율적인 배포를 위해 Github Action과 Docker를 사용하여 CI/CD를 진행하였습니다.
이를 위해 서버를 개발 서버와 운영서버로 분리하였고 운영서버의 경우는 도메인을 연결하였습니다.
Poetry를 사용하여 library 의존성을 관리했고, Docker Base Image를 생성하여 매번 사용하였습니다.
main branch의 server folder의 변경이 있을 경우 server에 접속,
pull 실행 후 다시 docker image를 bulid하여 run하는 방식으로 CI/CD를 구현 하였습니다.

다음은 모델링에 관한 내용입니다.
detection으로 East를
recognition으로 CRNN을 변형해서 사용하였습니다.
전체 모델 구조를 설명드리면
input image를 backbone을 통과시켜 feturemap을 뽑고
EAST 즉 detection 블록을 통해 bbox를 검출합니다.
검출된 bbox를 input image와 함께 roi align을 시켜
recognition을 수행할 이미지를 추출합니다.
추출된 이미지를 recognition을 통과시켜서 글자들을 검출해 줍니다.(넘겨)

recognition의 구조를 조금 더 디테일 하게 설명드리면
다음과 같이 convolution mish 및 convolution, Batch norm, mish block
 maxpool의 조합으로 feature map을 뽑아주고,
BiLinearLSTM을 통해 sequential한 예측을 진행하고
CTC loss를 통해 최종 output을 만들어 주었습니다.

마지막 층인 LSTM 과 CTC loss를 좀 더 구체적으로 설명 드리면 lstm을 통해
이미지를 그림에서와 같이 sequential하게 받을 것입니다.
그리고 여기서 HHHEEE와 같은 결과물이 나올 것이고 여기서
반복을 지우게 되면 l이 하나 빠진 helo가 나오게 됩니다.
하지만 슬라이드 에서와 같이 중간에 엡실론을 하나 추가하고 지워주게 되면
온전한 문장이 나옵니다.
여기서 CTC loss는 엡실론의 위치를 지정해주는 loss입니다.

말씀드린 부분 이외에도 
다른 모델 구조에 대한 실험, 앞의 feature을 줄여주는 과정을
detection의 backbone과 share하는 방법에 대한 실험
도 진행등도 진행하였으나
시간 관계상 질문으로 나오면 답변 드리도록 하겠습니다.(넘겨)

끝으로 목표치였던 기능을 구현했으며
목표 F1 score에 근접한 0.55달성 했으며
기능 구현 총 시간은 글자 수에 따라 다르긴 하지만
10단어 이내의 이미지에서 10초이내로 inference가 끝나게 구현이 완료 되었습니다.

영상 자막 번역 자동화를 통해 고생하시는 영상 편집자분들의
퇴근 시간을 구조해드리고 싶었던 컴퓨터 구조였습니다. 

감사합니다.